# -*- coding: utf-8 -*-
"""ScienceFair.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H7w4YaLL_3TPzNsxQMTlNhvx_NQOMnUx
"""

!rm -rf TrainingData
!rm TrainingData.zip
!rm class_names.json
!rm -rf sample_data

!gdown "1vzlp1alO92vfskKdueGOz2v04M-pyr5P" || pip install gdown && gdown "1vzlp1alO92vfskKdueGOz2v04M-pyr5P"

!unzip -F /content/TrainingData.zip && echo "Unzipped!"
!rm TrainingData.zip

"""Resize Images:"""

# Importing Image class from PIL module
from PIL import Image
import os
import json

training_path = "/content/TrainingData/"
dirs = [directory for directory in os.listdir(training_path)]

def process_images(dirs, im_path):
    for directory in dirs:
        for file in os.listdir(im_path + directory):
            # Opens a image in RGB mode
            im = Image.open(im_path + directory + "/" + file)
            im.thumbnail((256, 256))
            im.save(im_path + directory + "/" + file, optimize=True)

process_images(dirs, training_path)

"""Train a model:"""

import numpy as np
import os
import PIL
import PIL.Image
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import json

# Choose activation function
activation = 'sigmoid'
dataset_url = "/content/TrainingData/"

batch_size = 10
img_height = 256
img_width = 256

train_ds = tf.keras.utils.image_dataset_from_directory(
  dataset_url,
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
  dataset_url,
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

# Save class_names to json file
class_names = train_ds.class_names
print(class_names)

class_json = {}
for i in range(len(class_names)):
  class_json[i] = class_names[i]

with open("class_names.json", "w") as file:
  file.write(json.dumps(class_json, indent=4))

# Show images in training dataset
print("Training Dataset:")
plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")

# Show images in validation dataset
print("Validation Dataset:")
plt.figure(figsize=(10, 10))
for images, labels in val_ds.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")

for image_batch, labels_batch in train_ds:
  print(image_batch.shape)
  print(labels_batch.shape)
  break

# Preprocess
normalization_layer = tf.keras.layers.Rescaling(1./255)

normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(normalized_ds))
first_image = image_batch[0]
print(np.min(first_image), np.max(first_image))

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

num_classes = len([directory for directory in os.listdir(dataset_url)])

data_augmentation = tf.keras.Sequential([
  tf.keras.layers.RandomFlip("horizontal_and_vertical"),
  tf.keras.layers.RandomRotation(0.2),
  #tf.keras.layers.RandomZoom(0.2, 0.5)
])

# Create model
model = tf.keras.Sequential([
  data_augmentation,
  tf.keras.layers.Rescaling(1./255),
  tf.keras.layers.Conv2D(32, 3, activation=activation),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, activation=activation),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, activation=activation),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(5120, activation=activation),
  tf.keras.layers.Dense(num_classes)
])

model.compile(
  optimizer='adam',
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=50
)

model.save(f'{activation}_model.h5')

# Recreate the exact same model, including its weights and the optimizer
new_model = tf.keras.models.load_model(f'{activation}_model.h5')

# Show the model architecture
new_model.summary()

"""Make a prediction:"""

import tensorflow as tf
from PIL import Image
import numpy as np
import os
import json

# Select activation
activation = "sigmoid"

# Input data for testing image
# TODO: Automate this step for multiple images
img_path = "/content/testImages/hastalis/1.jpg"
img_dir = "/content/testImages/hastalis/"
img_class = "hastalis"

# Load model
model = tf.keras.models.load_model(f'{activation}_model.h5', compile = True)

img_width, img_height = 180, 180
image = tf.keras.preprocessing.image.load_img(img_path, target_size=(img_width, img_height))
image = tf.keras.preprocessing.image.img_to_array(image)
image = np.expand_dims(image, axis = 0)

prediction = model.predict(image)

predicted_class = np.argmax(prediction, axis=1)

# Load class data from data
with open("class_names.json", "r") as file:
  dirs = json.load(file)

percentages = tf.nn.softmax(prediction[0]).numpy() * 100

print(prediction)
print(["{:.20f}%".format(p) for p in percentages])
print(f"Predicted class: {dirs[str(predicted_class.tolist()[0])]}\nCorrect class: {img_class}")
if img_class == dirs[str(predicted_class.tolist()[0])]:
  print("Prediction Correct!")
else:
  print("Prediction Incorrect!")